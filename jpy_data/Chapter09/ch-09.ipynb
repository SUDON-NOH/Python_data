{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9장 : 문자 데이터와 소셜 미디어 분석하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어, 고유 명사, 숫자 걸러내기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영어 불용어 불러와 출력해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "print(\"Stop words:\", list(sw)[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구텐베르그 말뭉치 불러와 파일 이름 출력해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = nltk.corpus.gutenberg\n",
    "print(\"Gutenberg files:\\n\", gb.fileids()[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## milton-paradise.txt file에서 문장 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sent = gb.sents(\"milton-paradise.txt\")[:2]\n",
    "print(\"Unfiltered:\", text_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 추출된 문장에서 불용어 걸러내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in text_sent:\n",
    "    filtered = [w for w in sent if w.lower() not in sw]\n",
    "    print(\"Filtered:\\n\", filtered)\n",
    "    tagged = nltk.pos_tag(filtered)\n",
    "    print(\"Tagged:\\n\", tagged)\n",
    "\n",
    "    words= []\n",
    "    for word in tagged:\n",
    "        if word[1] != 'NNP' and word[1] != 'CD':\n",
    "           words.append(word[0]) \n",
    "\n",
    "    print(\"Words:\\n\",words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 주머니(bag-of-words) 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK 구텐베르크 말뭉치에서 2개의 문서 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hamlet = gb.raw(\"shakespeare-hamlet.txt\")\n",
    "macbeth = gb.raw(\"shakespeare-macbeth.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 제외하고 피쳐 벡터 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = sk.feature_extraction.text.CountVectorizer(stop_words='english')\n",
    "print(\"Feature vector:\\n\", cv.fit_transform([hamlet, macbeth]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두 문서에 해당하는 피쳐 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features:\\n\", cv.get_feature_names()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 빈도수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "gb = nltk.corpus.gutenberg\n",
    "words = gb.words(\"shakespeare-caesar.txt\")\n",
    "\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "filtered = [w.lower() for w in words if w.lower() not in sw and w.lower() not in punctuation]\n",
    "fd = nltk.FreqDist(filtered)\n",
    "print(\"Words\", list(fd.keys())[:5])\n",
    "print(\"Counts\", list(fd.values())[:5])\n",
    "print(\"Max\", fd.max())\n",
    "print(\"Count\", fd['d'])\n",
    "\n",
    "fd = nltk.FreqDist(nltk.bigrams(filtered))\n",
    "print(\"Bigrams\", list(fd.keys())[:5])\n",
    "print(\"Counts\", list(fd.values())[:5])\n",
    "print(\"Bigram Max\", fd.max())\n",
    "print(\"Bigram count\", fd[('let', 'vs')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나이브 베이즈 분류기(Naive Bayes classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import random\n",
    "\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def word_features(word):\n",
    "   return {'len': len(word)}\n",
    "\n",
    "def isStopword(word):\n",
    "    return word in sw or word in punctuation\n",
    "gb = nltk.corpus.gutenberg\n",
    "words = gb.words(\"shakespeare-caesar.txt\")\n",
    "\n",
    "labeled_words = ([(word.lower(), isStopword(word.lower())) for \n",
    "word in words])\n",
    "random.seed(42)\n",
    "random.shuffle(labeled_words)\n",
    "print(labeled_words[:5])\n",
    "\n",
    "featuresets = [(word_features(n), word) for (n, word) in \n",
    "labeled_words]\n",
    "cutoff = int(.9 * len(featuresets))\n",
    "train_set, test_set = featuresets[:cutoff], featuresets[cutoff:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"'behold' class\", classifier.classify(word_features('behold')))\n",
    "print(\"'the' class\", classifier.classify(word_features('the')))\n",
    "\n",
    "print(\"Accuracy\", nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "import string\n",
    "\n",
    "labeled_docs = [(list(movie_reviews.words(fid)), cat)\n",
    "        for cat in movie_reviews.categories()\n",
    "        for fid in movie_reviews.fileids(cat)]\n",
    "random.seed(42)\n",
    "random.shuffle(labeled_docs)\n",
    "\n",
    "review_words = movie_reviews.words()\n",
    "print(\"# Review Words\", len(review_words))\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word in sw or word in punctuation\n",
    "\n",
    "filtered = [w.lower() for w in review_words if not isStopWord(w.lower())]\n",
    "print(\"# After filter\", len(filtered))\n",
    "words = FreqDist(filtered)\n",
    "N = int(.05 * len(words.keys()))\n",
    "word_features = list(words.keys())[:N]\n",
    "\n",
    "def doc_features(doc):\n",
    "    doc_words = FreqDist(w for w in doc if not isStopWord(w))\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['count (%s)' % word] = (doc_words.get(word, 0))\n",
    "    return features\n",
    "\n",
    "featuresets = [(doc_features(d), c) for (d,c) in labeled_docs]\n",
    "train_set, test_set = featuresets[200:], featuresets[:200]\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "print(\"Accuracy\", accuracy(classifier, test_set))\n",
    "\n",
    "print(classifier.show_most_informative_features())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 워드 클라우드 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import string\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def isStopWord(word):\n",
    "    return word in sw or word in punctuation\n",
    "review_words = movie_reviews.words()\n",
    "filtered = [w.lower() for w in review_words if not isStopWord(w.lower())]\n",
    "\n",
    "words = FreqDist(filtered)\n",
    "N = int(.01 * len(words.keys()))\n",
    "tags = list(words.keys())[:N]\n",
    "\n",
    "for tag in tags:\n",
    "    print(tag, ':', words[tag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 빈도 및 역문서 빈도(Frequency-inverse document frequency, td-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import names\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "all_names = set([name.lower() for name in names.words()])\n",
    "\n",
    "def isStopWord(word):\n",
    "    return (word in sw or word in punctuation) or not word.isalpha() or word in all_names\n",
    "\n",
    "review_words = movie_reviews.words()\n",
    "filtered = [w.lower() for w in review_words if not isStopWord(w.lower())]\n",
    "\n",
    "words = FreqDist(filtered)\n",
    "\n",
    "texts = []\n",
    "\n",
    "for fid in movie_reviews.fileids():\n",
    "    texts.append(\" \".join([w.lower() for w in movie_reviews.words(fid) if not isStopWord(w.lower()) and words[w.lower()] > 1]))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "matrix = vectorizer.fit_transform(texts)\n",
    "sums = np.array(matrix.sum(axis=0)).ravel()\n",
    "\n",
    "ranks = []\n",
    "\n",
    "for word, val in zip(vectorizer.get_feature_names(), sums):\n",
    "    ranks.append((word, val))\n",
    "\n",
    "df = pd.DataFrame(ranks, columns=[\"term\", \"tfidf\"])\n",
    "df = df.sort_values(['tfidf'])\n",
    "print(df.head())\n",
    "\n",
    "N = int(.01 * len(df))\n",
    "df = df.tail(N)\n",
    "\n",
    "for term, tfidf in zip(df[\"term\"].values, df[\"tfidf\"].values):\n",
    "    print(term, \":\", tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소셜 미디어 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print([s for s in dir(nx) if s.endswith('graph')])\n",
    "G = nx.davis_southern_women_graph()\n",
    "plt.hist(list(nx.degree(G).values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(8,8))\n",
    "#pos = nx.spring_layout(G)\n",
    "#nx.draw(G, node_size=10)\n",
    "#nx.draw_networkx_labels(G, pos)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G)\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
