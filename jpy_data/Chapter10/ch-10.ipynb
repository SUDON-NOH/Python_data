{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측 분석과 기계 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리(Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import anderson\n",
    "\n",
    "rain = np.load('rain.npy')\n",
    "rain = .1 * rain\n",
    "rain[rain < 0] = .05/2\n",
    "print(\"Rain mean\", rain.mean())\n",
    "print(\"Rain variance\", rain.var())\n",
    "print(\"Anderson rain\", anderson(rain))\n",
    "\n",
    "scaled = preprocessing.scale(rain)\n",
    "print(\"Scaled mean\", scaled.mean())\n",
    "print(\"Scaled variance\", scaled.var())\n",
    "print(\"Anderson scaled\", anderson(scaled))\n",
    "\n",
    "binarized = preprocessing.binarize(rain.reshape(-1,1))\n",
    "print(np.unique(binarized), binarized.sum())\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(rain.astype(int))\n",
    "print(lb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로지스틱 회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "def classify(x, y):\n",
    "    clf = LogisticRegression(random_state=12)\n",
    "    scores = []\n",
    "    kf = KFold(n_splits=10)\n",
    "    for train,test in kf.split(x):\n",
    "      clf.fit(x[train], y[train])\n",
    "      scores.append(clf.score(x[test], y[test]))\n",
    "\n",
    "    print(\"Accuracy: \",np.mean(scores))\n",
    "\n",
    "rain = np.load('rain.npy')\n",
    "dates = np.load('doy.npy')\n",
    "\n",
    "x = np.vstack((dates[:-1], rain[:-1]))\n",
    "y = np.sign(rain[1:])\n",
    "classify(x.T, y)\n",
    "\n",
    "#iris 예제\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data[:, :2]\n",
    "y = iris.target\n",
    "classify(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지지도 벡터 머신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "def classify(x, y):\n",
    "    clf = GridSearchCV(SVC(random_state=42, max_iter=100), {'kernel': ['linear', 'poly', 'rbf'], 'C':[1, 10]})\n",
    "    clf.fit(x, y)\n",
    "    print(\"Accuracy: \", clf.score(x, y))\n",
    "    PrettyPrinter().pprint(clf.cv_results_)\n",
    "\n",
    "rain = np.load('rain.npy')\n",
    "dates = np.load('doy.npy')\n",
    "\n",
    "x = np.vstack((dates[:-1], rain[:-1]))\n",
    "y = np.sign(rain[1:])\n",
    "\n",
    "classify(x.T, y)\n",
    "\n",
    "#iris 예제\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data[:, :2]\n",
    "y = iris.target\n",
    "classify(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def regress(x, y, title):\n",
    "    clf = ElasticNetCV(max_iter=200, cv=10, l1_ratio = [.1, .5, \n",
    ".7, .9, .95, .99, 1])\n",
    "\n",
    "    clf.fit(x, y)\n",
    "    print(\"Score\", clf.score(x, y))\n",
    "\n",
    "    pred = clf.predict(x)\n",
    "    plt.title(\"Scatter plot of prediction and \" + title)\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.ylabel(\"Target\")\n",
    "    plt.scatter(y, pred)\n",
    "    # 완벽한 피팅 라인\n",
    "    if \"Boston\" in title:\n",
    "        plt.plot(y, y, label=\"Perfect Fit\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "rain = .1 * np.load('rain.npy')\n",
    "rain[rain < 0] = .05/2\n",
    "dates = np.load('doy.npy')\n",
    "\n",
    "x = np.vstack((dates[:-1], rain[:-1]))\n",
    "y = rain[1:]\n",
    "regress(x.T, y, \"rain data\")\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "regress(x, y, \"Boston house prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지지도 벡터 회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def regress(x, y, ncpus, title):\n",
    "    X = preprocessing.scale(x)\n",
    "    Y = preprocessing.scale(y)\n",
    "    clf = SVR(max_iter=ncpus * 200)\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, X, Y, n_jobs=ncpus) \n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.plot(train_sizes, train_scores.mean(axis=1), label=\"Train score\")\n",
    "    plt.plot(train_sizes, test_scores.mean(axis=1), '--', label=\"Test score\")\n",
    "    print(\"Max test score \" + title, test_scores.max())\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "rain = .1 * np.load('rain.npy')\n",
    "rain[rain < 0] = .05/2\n",
    "dates = np.load('doy.npy')\n",
    "\n",
    "x = np.vstack((dates[:-1], rain[:-1]))\n",
    "y = rain[1:]\n",
    "ncpus = multiprocessing.cpu_count()\n",
    "regress(x.T, y, ncpus, \"Rain\")\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "regress(x, y, ncpus, \"Boston\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 친근도 전파를 이용한 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import cluster\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "x, _ = datasets.make_blobs(n_samples=100, centers=3, n_features=2, \n",
    "random_state=10)\n",
    "S = euclidean_distances(x)\n",
    "\n",
    "aff_pro = cluster.AffinityPropagation().fit(S)\n",
    "labels = aff_pro.labels_\n",
    "\n",
    "styles = ['o', 'x', '^']\n",
    "\n",
    "for style, label in zip(styles, np.unique(labels)):\n",
    "   print(label)\n",
    "   plt.plot(x[labels == label], style, label=label)\n",
    "plt.title(\"Clustering Blobs\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평균 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import cluster\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "rain = .1 * np.load('rain.npy')\n",
    "rain[rain < 0] = .05/2\n",
    "dates = np.load('doy.npy')\n",
    "x = np.vstack((dates, rain))\n",
    "df = pd.DataFrame.from_records(x.T, columns=['dates', 'rain'])\n",
    "df = df.groupby('dates').mean()\n",
    "df.plot()\n",
    "x = np.vstack((np.arange(1, len(df) + 1) , \n",
    "df.as_matrix().ravel()))\n",
    "x = x.T\n",
    "ms = cluster.MeanShift()\n",
    "ms.fit(x)\n",
    "labels = ms.predict(x)\n",
    "\n",
    "plt.figure()\n",
    "grays = ['0', '0.5', '0.75']\n",
    "\n",
    "for gray, label in zip(grays, np.unique(labels)):\n",
    "    match = labels == label\n",
    "    x0 = x[:, 0]\n",
    "    x1 = x[:, 1]\n",
    "    plt.plot(x0[match], x1[match], lw=label+1, label=label)\n",
    "    plt.fill_between(x0, x1, where=match, color=gray)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유전자 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import array\n",
    "import random\n",
    "import numpy as np\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from scipy.stats import shapiro\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", array.array, typecode='d', \n",
    "fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", random.random)\n",
    "toolbox.register(\"individual\", tools.initRepeat, \n",
    "creator.Individual, toolbox.attr_float, 200)\n",
    "toolbox.register(\"populate\", tools.initRepeat, list, \n",
    "toolbox.individual)\n",
    "\n",
    "def eval(individual):\n",
    "    return shapiro(individual)[1],\n",
    "\n",
    "toolbox.register(\"evaluate\", eval)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=4)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "pop = toolbox.populate(n=400)\n",
    "hof = tools.HallOfFame(1)\n",
    "stats = tools.Statistics(key=lambda ind: ind.fitness.values)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=80, \n",
    "stats=stats, halloffame=hof)\n",
    "\n",
    "print(shapiro(hof[0])[1])\n",
    "plt.hist(hof[0])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theanets\n",
    "import multiprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rain = .1 * np.load('rain.npy')\n",
    "rain[rain < 0] = .05/2\n",
    "dates = np.load('doy.npy')\n",
    "x = np.vstack((dates[:-1], np.sign(rain[:-1])))\n",
    "x = x.T\n",
    "\n",
    "y = np.vstack(np.sign(rain[1:]),)\n",
    "N = int(.9 * len(x))\n",
    "\n",
    "train = [x[:N], y[:N]]\n",
    "valid = [x[N:], y[N:]]\n",
    "\n",
    "net = theanets.Regressor(layers=[2,3,1])\n",
    "\n",
    "net.train(train,valid, learning_rate=0.1,momentum=0.5)\n",
    "\n",
    "pred = net.predict(x[N:]).ravel()\n",
    "print(\"Pred Min\", pred.min(), \"Max\", pred.max())\n",
    "print(\"Y Min\", y.min(), \"Max\", y.max())\n",
    "print(\"Accuracy\", accuracy_score(y[N:], pred >= .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결정 트리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "import pydotplus as pydot\n",
    "import io\n",
    "import numpy as np\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "rain = .1 * np.load('rain.npy')\n",
    "rain[rain < 0] = .05/2\n",
    "dates = np.load('doy.npy').astype(int)\n",
    "x = np.vstack((dates[:-1], np.sign(rain[:-1])))\n",
    "x = x.T\n",
    "\n",
    "y = np.sign(rain[1:])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
    "random_state=37)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(random_state=37)\n",
    "params = {\"max_depth\": [2, None],\n",
    "              \"min_samples_leaf\": sp_randint(1, 5),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "rscv = RandomizedSearchCV(clf, params)\n",
    "rscv.fit(x_train,y_train)\n",
    "\n",
    "sio = io.StringIO()\n",
    "tree.export_graphviz(rscv.best_estimator_, out_file=sio, \n",
    "feature_names=['day-of-year','yest'])\n",
    "dec_tree = pydot.graph_from_dot_data(sio.getvalue())\n",
    "\n",
    "with NamedTemporaryFile(prefix='rain', suffix='.png', \n",
    "delete=False) as f:\n",
    "    dec_tree.write_png(f.name)\n",
    "    print(\"Written figure to\", f.name)\n",
    "\n",
    "print(\"Best Train Score\", rscv.best_score_)\n",
    "print(\"Test Score\", rscv.score(x_test, y_test))\n",
    "print(\"Best params\", rscv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
